{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example by hand :\n",
    "Question : Find the local minima of the function y=(x+5)² starting from the point x=3\n",
    "![graph](https://miro.medium.com/max/800/1*5-56UEwcZHgzqIAtlnsLog.png)\n",
    "Solution : We know the answer just by looking at the graph. y = (x+5)² reaches it’s minimum value when x = -5 (i.e when x=-5, y=0). Hence x=-5 is the local and global minima of the function.\n",
    "Now, let’s see how to obtain the same numerically using gradient descent.<br>\n",
    "Step 1 : Initialize x =3. Then, find the gradient of the function, dy/dx = 2*(x+5).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 : Move in the direction of the negative of the gradient (Why?). But wait, how much to move? For that, we require a learning rate. Let us assume the learning rate → 0.01 <br>\n",
    "Step 3 : Let’s perform 2 iterations of gradient descent<br>\n",
    "![](https://miro.medium.com/max/746/1*YkU1u_Px_FprYjKL1xtUwg.png)\n",
    "Step 4 : We can observe that the X value is slowly decreasing and should converge to -5 (the local minima). However, how many iterations should we perform?<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df(x):\n",
    "    return 2*(x+5)\n",
    "\n",
    "current_theta = 3 #start at x = 3 \n",
    "learning_rate = 0.1\n",
    "maximum_iteration = 500 #maximum no of iteration\n",
    "i = 0 #iteration counter\n",
    "value_difference = 1 #difference in value between two iterations\n",
    "precision = 0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 \n",
      " value is 1.4\n",
      "Iteration 2 \n",
      " value is 0.11999999999999966\n",
      "Iteration 3 \n",
      " value is -0.9040000000000001\n",
      "Iteration 4 \n",
      " value is -1.7232000000000003\n",
      "Iteration 5 \n",
      " value is -2.3785600000000002\n",
      "Iteration 6 \n",
      " value is -2.902848\n",
      "Iteration 7 \n",
      " value is -3.3222784\n",
      "Iteration 8 \n",
      " value is -3.65782272\n",
      "Iteration 9 \n",
      " value is -3.926258176\n",
      "Iteration 10 \n",
      " value is -4.1410065408\n",
      "Iteration 11 \n",
      " value is -4.312805232640001\n",
      "Iteration 12 \n",
      " value is -4.450244186112\n",
      "Iteration 13 \n",
      " value is -4.5601953488896\n",
      "Iteration 14 \n",
      " value is -4.64815627911168\n",
      "Iteration 15 \n",
      " value is -4.718525023289343\n",
      "Iteration 16 \n",
      " value is -4.774820018631475\n",
      "Iteration 17 \n",
      " value is -4.81985601490518\n",
      "Iteration 18 \n",
      " value is -4.855884811924144\n",
      "Iteration 19 \n",
      " value is -4.884707849539315\n",
      "Iteration 20 \n",
      " value is -4.907766279631452\n",
      "Iteration 21 \n",
      " value is -4.926213023705161\n",
      "Iteration 22 \n",
      " value is -4.940970418964129\n",
      "Iteration 23 \n",
      " value is -4.952776335171303\n",
      "Iteration 24 \n",
      " value is -4.962221068137042\n",
      "Iteration 25 \n",
      " value is -4.969776854509634\n",
      "Iteration 26 \n",
      " value is -4.9758214836077075\n",
      "Iteration 27 \n",
      " value is -4.980657186886166\n",
      "Iteration 28 \n",
      " value is -4.984525749508933\n",
      "Iteration 29 \n",
      " value is -4.987620599607146\n",
      "Iteration 30 \n",
      " value is -4.990096479685716\n",
      "Iteration 31 \n",
      " value is -4.992077183748573\n",
      "Iteration 32 \n",
      " value is -4.993661746998859\n",
      "Iteration 33 \n",
      " value is -4.994929397599087\n",
      "Iteration 34 \n",
      " value is -4.99594351807927\n",
      "Iteration 35 \n",
      " value is -4.996754814463416\n",
      "Iteration 36 \n",
      " value is -4.997403851570732\n",
      "Iteration 37 \n",
      " value is -4.997923081256586\n",
      "Iteration 38 \n",
      " value is -4.9983384650052685\n",
      "Iteration 39 \n",
      " value is -4.998670772004215\n",
      "Iteration 40 \n",
      " value is -4.998936617603372\n",
      "Iteration 41 \n",
      " value is -4.999149294082697\n",
      "Iteration 42 \n",
      " value is -4.999319435266157\n",
      "Iteration 43 \n",
      " value is -4.999455548212926\n",
      "Iteration 44 \n",
      " value is -4.99956443857034\n",
      "Iteration 45 \n",
      " value is -4.999651550856273\n",
      "Iteration 46 \n",
      " value is -4.999721240685018\n",
      "Iteration 47 \n",
      " value is -4.999776992548014\n",
      "Iteration 48 \n",
      " value is -4.999821594038411\n",
      "Iteration 49 \n",
      " value is -4.999857275230729\n",
      "Iteration 50 \n",
      " value is -4.999885820184583\n",
      "Iteration 51 \n",
      " value is -4.999908656147666\n",
      "Iteration 52 \n",
      " value is -4.9999269249181335\n",
      "Iteration 53 \n",
      " value is -4.999941539934507\n",
      "Iteration 54 \n",
      " value is -4.999953231947606\n",
      "Iteration 55 \n",
      " value is -4.999962585558085\n",
      "Iteration 56 \n",
      " value is -4.999970068446467\n",
      "Iteration 57 \n",
      " value is -4.999976054757174\n",
      "Iteration 58 \n",
      " value is -4.999980843805739\n",
      "Iteration 59 \n",
      " value is -4.999984675044591\n",
      "Iteration 60 \n",
      " value is -4.999987740035673\n",
      "Iteration 61 \n",
      " value is -4.999990192028538\n",
      "Iteration 62 \n",
      " value is -4.999992153622831\n",
      "Iteration 63 \n",
      " value is -4.999993722898265\n",
      "Iteration 64 \n",
      " value is -4.999994978318612\n",
      "Iteration 65 \n",
      " value is -4.999995982654889\n",
      "Iteration 66 \n",
      " value is -4.999996786123911\n",
      "The local minimum occurs at -4.999996786123911\n"
     ]
    }
   ],
   "source": [
    "while value_difference > precision and i < maximum_iteration:\n",
    "    old_theta = current_theta #store current x value in old theta\n",
    "    current_theta = current_theta - learning_rate * df(old_theta) #gradient descent\n",
    "    value_difference = abs(current_theta-old_theta) #change of x\n",
    "    i+=1 #iteration_count\n",
    "    print(\"Iteration\",i,\"\\n value is\",current_theta) #print iterations\n",
    "\n",
    "print(\"The local minimum occurs at\", current_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving Linear Equation using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 2 * np.random.rand(10,1)\n",
    "Y = 4 + 3 * X + np.random.randn(10,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_cost(theta,X,Y):\n",
    "    m=len(Y)\n",
    "    prediction = X.dot(theta)\n",
    "    cost = (1/(2*m)) * np.sum(np.square(prediction-Y))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X,Y,theta,learning_rate=0.01,iterations=5000):\n",
    "    m=len(Y)\n",
    "    cost_history = np.zeros(iterations)\n",
    "    theta_history = np.zeros((iterations,2))\n",
    "    for it in range(iterations):\n",
    "        prediction = np.dot(X,theta)\n",
    "        theta = theta-(1/m)*learning_rate*(X.T.dot((prediction - Y)))\n",
    "        theta_history[it,:] = theta.T\n",
    "        cost_history[it] = cal_cost(theta,X,Y)\n",
    "    return theta_history[np.argmin(cost_history)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.11240534 2.03253828]\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "n_iter =2000\n",
    "theta = np.random.randn(2,1)\n",
    "X_b = np.c_[np.ones((len(X),1)),X]\n",
    "theta_optimum = gradient_descent(X_b,Y,theta,lr,n_iter)\n",
    "print(theta_optimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Linear Equation using Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stocashtic_gradient_descent(X,Y,theta,learning_rate=0.01,iterations = 10):\n",
    "    m=len(Y)\n",
    "    for it in range(iterations):\n",
    "        for i in range(m):\n",
    "            rand_ind = np.random.randint(0,m)\n",
    "            X_i=X[rand_ind,:].reshape(1,X.shape[1])\n",
    "            Y_i=Y[rand_ind].reshape(1,1)\n",
    "            prediction = np.dot(X_i,theta)\n",
    "            theta = theta - learning_rate * (X_i.T.dot((prediction-Y_i)))\n",
    "    return theta        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.13613295]\n",
      " [1.9933342 ]]\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "n_iter = 2000\n",
    "theta = np.random.randn(2,1)\n",
    "X_b = np.c_[np.ones((len(X),1)),X]\n",
    "theta_optimum = stocashtic_gradient_descent(X_b,Y,theta,lr,n_iter)\n",
    "print(theta_optimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Linear Equation using Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_gradient_descent(X,Y,theta,learning_rate=0.01,iterations = 10, batch_size=20):\n",
    "    m=len(Y)\n",
    "    n_batches = int(m/batch_size)\n",
    "    \n",
    "    for it in range(iterations):\n",
    "        indices = np.random.permutation(m)\n",
    "        X = X[indices]\n",
    "        Y = Y[indices]\n",
    "        for i in range(0,m,batch_size):\n",
    "            X_i = X[i:i+batch_size]\n",
    "            Y_i = Y[i:i+batch_size]\n",
    "            \n",
    "            prediction = np.dot(X_i,theta)\n",
    "            theta = theta - (1/m) * learning_rate * (X_i.T.dot((prediction - Y_i)))\n",
    "    return theta        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.31600142]\n",
      " [2.67943491]]\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "n_iter = 200\n",
    "theta = np.random.randn(2,1)\n",
    "X_b = np.c_[np.ones((len(X),1)),X]\n",
    "theta_optimum = minibatch_gradient_descent(X_b,Y,theta,lr,n_iter)\n",
    "print(theta_optimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
